# -*- coding: utf-8 -*-
"""Final_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qMkvB_xJtJdSTrMD31iUE8mKxLU107DQ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
nltk.download('punkt')
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
!pip install wordcloud
from wordcloud import WordCloud
nltk.download("stopwords")
nltk.download("wordnet")
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

!pip install scikit-plot
from scikitplot.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve

!pip install --upgrade scikit-learn

!pip install pattern

from google.colab import drive
drive.mount('/content/drive')

# importing spam tweet dataset
dfa = pd.read_csv("/content/drive/MyDrive/training.1600000.processed.noemoticon.csv", header=None, encoding='latin-1')
dfa.head(10)

# assigning the column names and taking a fraction of complete dataset
df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']
df = df.sample(frac=0.6).reset_index(drop=True)
df['text'] = df['text'].apply(str)

#converting the target variable into categorical by assigning 0 as spam and 1 as non-spam
dfa['spam'] = dfa['target'].apply(lambda x: 'spam' if x == 0 else 'non-spam')
display(dfa)

dfa.drop("target",axis=1,inplace=True)

# text pre-processing using NLP
from pattern.en import lemma
from nltk.corpus import stopwords

def transformation(column):
    output = []
    for i in column:
        text = re.sub(r"#\w+\s*|(#\w+\S+)", " ", str(i))
        text = re.sub("@\w+\S*|&\S*|\b\w+\d+\S*|http\S+|www\S+|https\S+|[^a-zA-Z]"," ", text)
        text = re.sub(r"\b(\w{1,2})\b", lambda match: lemma(match.group()), text)
        text = re.sub(r"\s+", " ", text)
        text = text.lower()
        tokens = text.split()
        lemmatized_tokens = [token for token in tokens if token not in stopwords.words("english") and len(token) > 2]
        output.append(" ".join(lemmatized_tokens))
    return output

def lemmatize_tweets(tweets):
    lemmatized_tweets = []
    for tweet in tweets:
        tokens = nltk.word_tokenize(tweet)
        lemmatized_tokens = []
        for token in tokens:
            if token.endswith('s') and len(token) > 1:
                lemmatized_tokens.append(token)
            else:
                lemmatized_tokens.append(lemma(token))
        lemmatized_tweet = " ".join(lemmatized_tokens)
        lemmatized_tweets.append(lemmatized_tweet)
    return lemmatized_tweets

transformed_data1 = transformation(df.text)
lemmatized_tweets = lemmatize_tweets(transformed_data1)
df["processed_tweets"]=lemmatized_tweets

# deleting the blank cells
df = df[df['processed_tweets2'].str.strip().astype(bool)]
display(df)

label_encoder = LabelEncoder()
dfa["y_encoded"] = label_encoder.fit_transform(dfa.spam)
dfa

# splitting the spam tweets dataset
x1 = dfa['processed_tweets'].values
y1 = dfa['y_encoded'].values
x_train, x_test, y_train, y_test= train_test_split(x1,y1, test_size=0.3, random_state=42)

print(x.shape)
print(y.shape)

print(x_test.shape)
print(x_train.shape)

# vectorising x_train and x_test
from sklearn.feature_extraction.text import TfidfVectorizer
en_stopwords = stopwords.words("english")
vectorizer = TfidfVectorizer(
    analyzer='word',
    lowercase=True,
    ngram_range=(1, 1),
    stop_words=en_stopwords
)

vectorizer.fit(x_train)
x_train_vectorized = vectorizer.transform(x_train)
x_test_vectorized = vectorizer.transform(x_test)

# Using function
def model_Evaluate(model):
    acc_train=model.score(x_train_vectorized, y_train)
    acc_test=model.score(x_test_vectorized, y_test)
    print('Accuracy of model on training data : {}'.format(acc_train*100))
    print('Accuracy of model on testing data : {} \n'.format(acc_test*100))
    y_pred = model.predict(x_test_vectorized)
    print(classification_report(y_test, y_pred))
    categories = [ 'Non-spam','Spam']
    cf_matrix = confusion_matrix(y_test, y_pred)
    labels = np.asarray(cf_matrix.flatten()).reshape(2, 2)
    sns.heatmap(cf_matrix, annot=labels, cmap='Reds', fmt='', xticklabels=categories, yticklabels=categories)
    plt.xlabel("Predicted labels")
    plt.ylabel("True labels")
    plt.title("Confusion Matrix")
    plt.show()

# without using function
lg = LogisticRegression()
history=lg.fit(x_train_vectorized, y_train)

acc_train=lg.score(x_train_vectorized, y_train)
acc_test=lg.score(x_test_vectorized, y_test)
print('Accuracy of model on training data : {}'.format(acc_train*100))
print('Accuracy of model on testing data : {} \n'.format(acc_test*100))

y_pred1 = lg.predict(x_test_vectorized)
print(classification_report(y_test, y_pred1))
cf_matrix = confusion_matrix(y_test, y_pred1)

categories  = ['Negative','Positive']
group_names = ['True Neg','False Pos', 'False Neg','True Pos']
group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]

labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '', xticklabels = categories, yticklabels = categories)

plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
plt.ylabel("Actual values"   , fontdict = {'size':14}, labelpad = 10)
plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

rf = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', max_depth=50)
rf.fit(x_train_vectorized, y_train)

acc_train=rf.score(x_train_vectorized, y_train)
acc_test=rf.score(x_test_vectorized, y_test)
print('Accuracy of model on training data : {}'.format(acc_train*100))
print('Accuracy of model on testing data : {} \n'.format(acc_test*100))

y_pred2 = rf.predict(x_test_vectorized)
print(classification_report(y_test, y_pred2))
cf_matrix = confusion_matrix(y_test, y_pred2)

categories  = ['Negative','Positive']
group_names = ['True Neg','False Pos', 'False Neg','True Pos']
group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]

labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '',xticklabels = categories, yticklabels = categories)

plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
plt.ylabel("Actual values"   , fontdict = {'size':14}, labelpad = 10)
plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

svm = LinearSVC()
svm.fit(x_train_vectorized, y_train)

acc_train=svm.score(x_train_vectorized, y_train)
acc_test=svm.score(x_test_vectorized, y_test)
print('Accuracy of model on training data : {}'.format(acc_train*100))
print('Accuracy of model on testing data : {} \n'.format(acc_test*100))

y_pred3 = svm.predict(x_test_vectorized)
print(classification_report(y_test, y_pred3))
cf_matrix = confusion_matrix(y_test, y_pred3)

categories  = ['Negative','Positive']
group_names = ['True Neg','False Pos', 'False Neg','True Pos']
group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]

labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '',
                xticklabels = categories, yticklabels = categories)

plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
plt.ylabel("Actual values"   , fontdict = {'size':14}, labelpad = 10)
plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
nb = MultinomialNB()
nb.fit(x_train_vectorized, y_train)
acc_train=nb.score(x_train_vectorized, y_train)
acc_test=nb.score(x_test_vectorized, y_test)
print('Accuracy of model on training data : {}'.format(acc_train*100))
print('Accuracy of model on testing data : {} \n'.format(acc_test*100))

y_pred4 = nb.predict(x_test_vectorized)
print(classification_report(y_test, y_pred4))

cf_matrix = confusion_matrix(y_test, y_pred4)

categories  = ['Negative','Positive']
group_names = ['True Neg','False Pos', 'False Neg','True Pos']
group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]

labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '',
                xticklabels = categories, yticklabels = categories)

plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
plt.ylabel("Actual values"   , fontdict = {'size':14}, labelpad = 10)
plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

# plotting learning curve
from sklearn.model_selection import learning_curve
model = LinearSVC()
x_vectorized = vectorizer.transform(x1)
train_sizes, train_scores, val_scores = learning_curve(model, x_vectorized, y1, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training accuracy')
plt.plot(train_sizes, val_mean, label='Validation accuracy')
plt.fill_between(train_sizes,train_mean - train_std,train_mean + train_std,alpha=0.3)
plt.fill_between(train_sizes,val_mean - val_std,val_mean + val_std,alpha=0.3)
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.title('Learning Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# creating ensemble of these 4 model predictions
pred1 = lg.predict(x_test_vectorized)
pred2 = rf.predict(x_test_vectorized)
pred3 = svm.predict(x_test_vectorized)
pred4 = nb.predict(x_test_vectorized)

ensemble_predictions = []
for i in range(len(pred1)):
    votes = [pred1[i], pred2[i], pred3[i], pred4[i]]
    spam_votes = sum(votes)
    if spam_votes >= 3:
        ensemble_predictions.append(1)
    else:
        ensemble_predictions.append(0)
ensemble_df = pd.DataFrame({'Final Prediction': ensemble_predictions})

en_df = pd.DataFrame({'LR': pred1,"Rf":pred2,"SVM":pred3,"Nb":pred4})
en_df

ensemble_df

df=pd.read_csv("/content/drive/MyDrive/tweets - 01.06-MOSPI.csv")
df.head()

df.nunique()

df=df.drop_duplicates(subset=['Tweet'])
df

from pattern.en import lemma
from nltk.corpus import stopwords

def transformation(column):
    output = []
    for i in column:
        text = re.sub(r"#\w+\s*|(#\w+\S+)", " ", str(i))
        text = re.sub("@\w+\S*|&\S*|\b\w+\d+\S*|http\S+|www\S+|https\S+|[^a-zA-Z]"," ", text)
        text = re.sub(r"\b(\w{1,2})\b", lambda match: lemma(match.group()), text)
        text = re.sub(r"\s+", " ", text)
        text = text.lower()
        tokens = text.split()
        lemmatized_tokens = [token for token in tokens if token not in stopwords.words("english") and len(token) > 2]
        output.append(" ".join(lemmatized_tokens))
    return output

def lemmatize_tweets(tweets):
    lemmatized_tweets = []
    for tweet in tweets:
        tokens = nltk.word_tokenize(tweet)
        lemmatized_tokens = []
        for token in tokens:
            if token.endswith('s') and len(token) > 1:
                lemmatized_tokens.append(token)
            else:
                lemmatized_tokens.append(lemma(token))
        lemmatized_tweet = " ".join(lemmatized_tokens)
        lemmatized_tweets.append(lemmatized_tweet)
    return lemmatized_tweets

transformed_data1 = transformation(df.text)
lemmatized_tweets = lemmatize_tweets(transformed_data1)
df["processed_tweets"]=lemmatized_tweets

df = df[df['New_tweet'].str.strip().astype(bool)]
display(df)

df.reset_index(drop=True,inplace=True)
df

# count the number of words in cleaned tweets
from nltk.tokenize import word_tokenize
df['words in text']=df['New_tweet'].apply(lambda x:len(word_tokenize(x)))
df

# considering only those tweets which have word count>8
newdf= combined_df[combined_df['words in text']>8]
newdf.reset_index(drop=True,inplace=True)
newdf

df=df.drop_duplicates(subset=['New_tweet'])
df.reset_index(inplace=True)
df

from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
vectorizer = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1, 1),stop_words=en_stopwords)
vectorizer.fit(x_train)

x_train_vectorized = vectorizer.transform(x_train)
x_test_vectorized = vectorizer.transform(x_test)

max_length = 144968
x_train_padded = pad_sequences(x_train_vectorized.toarray(), maxlen=max_length)
x_test_padded = pad_sequences(x_test_vectorized.toarray(), maxlen=max_length)

ensemble_predictions = np.vstack((pred1, pred2, pred3, pred4)).T
ensemble_model = VotingClassifier(estimators=[('ensemble', ensemble_predictions)], voting='hard')

x_padded = pad_sequences(x_vectorized.toarray(), maxlen=max_length)
ensemble_predictions = ensemble_model.predict(x_padded)

labels = []
for prediction in ensemble_predictions:
    if prediction == 1:
        labels.append("spam")
    else:
        labels.append("non-spam")
classified_tweets = pd.DataFrame({"Tweet": df["New_tweet"], "Label": labels})

combined_df = pd.concat([df, classified_tweets['Label']], axis=1)
display(combined_df)

combined_df = pd.concat([df, classified_tweets['Label']], axis=1)
newdf= combined_df[combined_df['words in text']>8]
newdf.reset_index(drop=True,inplace=True)
newdf

# Printing number of spam and ham tweets
newdf['Label'].value_counts()

# Plot the pie chart showing spam and ham tweets' count
spam_count = df1[df1['Label'] == 'spam'].shape[0]
non_spam_count = df1[df1['Label'] == 'non-spam'].shape[0]
total_count = spam_count + non_spam_count
spam_percentage = (spam_count / total_count) * 100
non_spam_percentage = (non_spam_count / total_count) * 100
labels = ['Spam', 'Non-Spam']
sizes = [spam_percentage, non_spam_percentage]
colors = ['red', 'yellow']
explode = (0.1, 0)
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, explode=explode)
plt.title('Percentage of Spam and Non-Spam Tweets')
plt.axis('equal')
plt.show()

# calling the tweets which are spam
spam_tweets = df[df['Label'] == 'spam']
spam_df = spam_tweets[["UserName",'Tweet',"New_tweet", 'Label']].copy()
spam_df['Index'] = spam_tweets.index
display(spam_df)

# creating a list of spam words
spam_words = ['buy', 'video',"review", 'webinar',"disclaimer","entertainment","gold","call","whats app","watch","episode","helium","join","latest updates","update","news","read"]
pattern = '|'.join(spam_words)
spam_tweets = spam_df[spam_df['New_tweet'].str.contains(pattern, case=False)]
display(spam_tweets_df)

# deleting spam tweets
df = df.drop([3, 17, 29,30,36,175,183,227,262,254,264,270,390,402,407,542,619,676,677,697,772,784,831,1230,1243,1410,1502,1581,1624,1793,1804,2045,2077,2113,2118,2182,2263,2310,2316,2322,2332,2335,2379,2404,2416,2420,2441,2448])
df

df.info()

!pip install transformers
!pip install tensorflow

# MODEL-1
from transformers import pipeline
SentimentClassifier = pipeline("sentiment-analysis")

sentiments = []
scores = []

for index, row in df.iterrows():
    text = row['New_tweet']
    result = SentimentClassifier(text)[0]
    sentiments.append(result['label'])
    scores.append(result['score'])

df['label'] = sentiments
df['score'] = scores
display(df)

# MODEL-2
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

sentiments = []
scores = []

for index, row in df.iterrows():
    text = row['New_tweet']
    result = SentimentClassifier(text)[0]
    sentiments.append(result['label'])
    scores.append(result['score'])

df['label'] = sentiments
df['score'] = scores
display(df)

label_mapping = {
    '1 star': 'Negative',
    '2 stars': 'Negative',
    '3 stars': 'Neutral',
    '4 stars': 'Positive',
    '5 stars': 'Positive'
}
for index, row in df2.iterrows():
    label = row['label']
    sentiment = label_mapping.get(label)
    df2.at[index, 'sentiment'] = sentiment
display(df2)

# MODEL-3
model_name = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

sentiments = []
scores = []

for index, row in df.iterrows():
    text = row['New_tweet']
    result = SentimentClassifier(text)[0]
    sentiments.append(result['label'])
    scores.append(result['score'])

df['label'] = sentiments
df['score'] = scores
display(df)

# MODEL-4
def polarity(text):
  return TextBlob(text).sentiment.polarity

df['polarity'] = df['New_tweet'].apply(polarity)

def sentiment(label):
  if label <0:
      return "Negative"
  elif label ==0:
    return "Neutral"
  elif label >0:
    return "Positive"

df['sentiment'] = df['polarity'].apply(sentiment)
df

# MODEL-5
from transformers import ElectraTokenizer, TFElectraForSequenceClassification
model_name = "google/electra-base-discriminator"
model = TFElectraForSequenceClassification.from_pretrained(model_name)
tokenizer = ElectraTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, framework='tf')

sentiments = []
scores = []

for index, row in df.iterrows():
    text = row['New_tweet']
    result = SentimentClassifier(text)[0]
    sentiments.append(result['label'])
    scores.append(result['score'])

df['label'] = sentiments
df['score'] = scores
display(df)

label_mapping = {
    'LABEL_0': 'Negative',
    'LABEL_1': 'Positive'
}
for index, row in df5.iterrows():
    label = row['label']
    sentiment = label_mapping.get(label)
    df5.at[index, 'sentiment'] = sentiment
display(df5)

# creating an ensemble of these 5 models
sub_all=pd.DataFrame({'Model_polarity':df4.sentiment,'Model_electra':df5.sentiment,'Model_xlm-roberta':df6.label,"Model_distilbert":df1.label,'Model_bert-base':df2.sentiment})
pred_mode=sub_all.agg('mode',axis=1)[0].values
sub_all

# Plot the grouped bar graph
df_cleaned = df.applymap(lambda x: str(x).strip().lower())
grouped_data = df_cleaned.apply(pd.value_counts).fillna(0).T
sentiments = grouped_data.index
columns = grouped_data.columns
bar_width = 0.2
r = np.arange(len(sentiments))
bar_positions = [r + i * bar_width for i in range(len(columns))]
for i, col in enumerate(columns):
    plt.bar(bar_positions[i], grouped_data[col], width=bar_width, label=col)

plt.xticks(r + bar_width * (len(columns) - 1) / 2, sentiments)
plt.ylabel('Count')
plt.legend()
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
columns_to_encode = ['Model_polarity', 'Model_electra', 'Model_xlm-roberta', 'Model_distilbert', 'Model_bert-base']
for column in columns_to_encode:
    label_encoder.fit(df[column])
    df[column] = label_encoder.transform(df[column])
display(df)

df['Mode'] = df.mode(axis=1)[0]
df

label_mapping = {
    0.0: 'Negative',1.0: 'Neutral',2.0: 'Positive'
}
for index, row in df.iterrows():
    label = row['Mode']
    sentiment = label_mapping.get(label)
    df.at[index, 'sentiment'] = sentiment
display(df)

df.drop(df.columns[0:6],axis=1,inplace=True)

df.to_excel('Final_Sentiments.xlsx', index=False)





def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus = get_corpus(newdf1.New_tweet)
corpus[:11]

# plot the graph for most common words
from collections import Counter
counter = Counter(corpus)
most_common = counter.most_common(17)
most_common = pd.DataFrame(most_common,columns = ['corpus','countv'])

most_common = most_common.sort_values('countv')
plt.figure(figsize =(10,10))
plt.yticks(range(len(most_common)), list(most_common.corpus))
plt.barh(range(len(most_common)), list(most_common.countv),align='center',height=0.5,color = 'cyan')
plt.title('Most common words in the dataset')
plt.grid()
plt.show()

#For wordcloud
from wordcloud import WordCloud
text = ' '.join(newdf1['New_tweet'])
wordcloud = WordCloud(
    max_words=500,
    width=1600,
    height=800,
    background_color="black",
    min_font_size=10,
    collocations=False
).generate(text)
plt.figure(figsize=(20, 15))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()



# Importing both final_sentiment file and original dataset having cleaned tweets file and concatenate them to get final df
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

columns_to_encode = ["sentiment"]
for column in columns_to_encode:
    label_encoder.fit(df[column])
    df[column] = label_encoder.transform(df[column])

display(df)

df['sentiment'].value_counts()

#Graph for final sentiments
sentiment_counts = df['sentiment'].value_counts()
labels = ['Negative', 'Neutral','Positive']
colors = ['red', 'blue', 'yellow']
plt.figure(figsize=(8, 6))
bars = plt.bar(sentiment_counts.index, sentiment_counts.values, color=colors)
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.title('Sentiment Distribution')
plt.xticks(sentiment_counts.index, labels)
plt.grid()
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
en_stopwords = stopwords.words("english")
vectorizer = TfidfVectorizer(
    analyzer='word',
    lowercase=True,
    ngram_range=(1, 1),
    stop_words=en_stopwords
)
vectorizer.fit(x)
x_vectorized = vectorizer.transform(x)

# Using resampling technique SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE(sampling_strategy='minority')
x_resampled, y_resampled = smote.fit_resample(x_vectorized, y)
unique, counts = np.unique(y_resampled, return_counts=True)
class_distribution = dict(zip(unique, counts))
print(class_distribution)

sentiment_counts = df['sentiment'].value_counts()
labels = ['Negative', 'Neutral','Positive']
colors = ['red', 'blue', 'yellow']
plt.figure(figsize=(8, 6))
bars = plt.bar(sentiment_counts.index, sentiment_counts.values, color=colors)
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.title('Sentiment Distribution')
plt.xticks(sentiment_counts.index, labels)
plt.grid()
plt.show()



"""Using ML models"""

# Splitting the data
train,test=train_test_split(df,test_size=0.2, random_state=42)
x_train = train['New_tweet'].values
x_test = test['New_tweet'].values
y_train = train['sentiment']
y_test = test['sentiment']

en_stopwords = stopwords.words("english")
vectorizer = TfidfVectorizer(
    analyzer='word',
    lowercase=True,
    ngram_range=(1, 1),
    stop_words=en_stopwords
)
vectorizer.fit(x_train)
x_train_vectorized = vectorizer.transform(x_train)
x_test_vectorized = vectorizer.transform(x_test)

# using SVM
from sklearn.metrics import classification_report
kfolds = StratifiedKFold(n_splits=15, shuffle=True, random_state=36)
np.random.seed(1)

pipeline_svm = make_pipeline(LinearSVC(class_weight="balanced"))

grid_svm = GridSearchCV(pipeline_svm,param_grid={'linearsvc__C': [0.01, 0.1, 0.06],'linearsvc__penalty': ['l2'],'linearsvc__loss': ['squared_hinge'],
                                                 'linearsvc__dual': [False, True]},cv=kfolds,scoring="accuracy",verbose=2,n_jobs=-1)

grid_svm.fit(x_train_vectorized, y_train)
train_predictions = grid_svm.predict(x_train_vectorized)
test_predictions = grid_svm.predict(x_test_vectorized)
print("Classification Report - Training Data:")
print(classification_report(y_train, train_predictions))
print("Classification Report - Test Data:")
print(classification_report(y_test, test_predictions))

train_accuracy = grid_svm.score(x_train_vectorized, y_train)
test_accuracy = grid_svm.score(x_test_vectorized, y_test)
print(grid_svm.best_params_)
print("Accuracy - Training Data:", train_accuracy)
print("Accuracy - Test Data:", test_accuracy)

# before resampling cm
categories = ['Negative',  'Neutral','Positive']
cf_matrix = confusion_matrix(y_test, test_predictions)
labels = np.asarray(cf_matrix.flatten()).reshape(3, 3)
sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt='', xticklabels=categories, yticklabels=categories)
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.title("Confusion Matrix")
plt.show()

# after resampling
from sklearn.naive_bayes import LinearSVC
kfolds = StratifiedKFold(n_splits=15, shuffle=True, random_state=36)
x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=42)

pipeline_svm = make_pipeline(LinearSVC(class_weight="balanced"))

grid_svm = GridSearchCV(
    pipeline_svm,
    param_grid={
        'linearsvc__C': [0.01, 0.1, 0.06],
        'linearsvc__penalty': ['l2'],
        'linearsvc__loss': ['squared_hinge'],
        'linearsvc__dual': [False, True]
    },
    cv=kfolds,
    scoring="accuracy",
    verbose=2,
    n_jobs=-1
)
grid_svm.fit(x_train, y_train)

train_predictions = grid_svm.predict(x_train)
test_predictions = grid_svm.predict(x_test)

print("Classification Report - Training Data:")
print(classification_report(y_train, train_predictions))

print("Classification Report - Test Data:")
print(classification_report(y_test, test_predictions))

# after resampling cm
categories = ['Negative',  'Neutral','Positive']
cf_matrix = confusion_matrix(y_test, test_predictions)
labels = np.asarray(cf_matrix.flatten()).reshape(3, 3)
sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt='', xticklabels=categories, yticklabels=categories)
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.title("Confusion Matrix")
plt.show()

en_stopwords = stopwords.words("english")
vectorizer = TfidfVectorizer(
    analyzer='word',
    lowercase=True,
    ngram_range=(1, 1),
    stop_words=en_stopwords
)
vectorizer.fit(x)
x_vectorized = vectorizer.transform(x)

# plot learning curves
model = LinearSVC()
x_vectorized = vectorizer.transform(x)
train_sizes, train_scores, val_scores = learning_curve(
    model, x_resampled, y_resampled, cv=kfolds, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)
)
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training accuracy')
plt.plot(train_sizes, val_mean, label='Testing accuracy')

plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.title('Learning Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# using multinomial
from sklearn.naive_bayes import MultinomialNB
pipeline_nb = make_pipeline(MultinomialNB())

kfolds = StratifiedKFold(n_splits=15, shuffle=True, random_state=36)
grid_nb = GridSearchCV(pipeline_nb,param_grid={'multinomialnb__alpha': [0.01, 0.1, 0.06],'multinomialnb__fit_prior': [True, False]},
                       cv=kfolds,scoring="accuracy",verbose=2,n_jobs=-1)

grid_nb.fit(x_train_vectorized, y_train)
train_predictions = grid_nb.predict(x_train_vectorized)
test_predictions = grid_nb.predict(x_test_vectorized)

print("Classification Report - Training Data:")
print(classification_report(y_train, train_predictions))

print("Classification Report - Test Data:")
print(classification_report(y_test, test_predictions))

train_accuracy = grid_nb.score(x_train_vectorized, y_train)
test_accuracy = grid_nb.score(x_test_vectorized, y_test)
print(grid_nb.best_params_)
print("Accuracy - Training Data:", train_accuracy)
print("Accuracy - Test Data:", test_accuracy)

from sklearn.metrics import confusion_matrix
categories = ['Negative',  'Neutral','Positive']
cf_matrix = confusion_matrix(y_test, test_predictions)
labels = np.asarray(cf_matrix.flatten()).reshape(3, 3)
sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt='', xticklabels=categories, yticklabels=categories)
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.title("Confusion Matrix")
plt.show()

# after resampling
from sklearn.naive_bayes import MultinomialNB
kfolds = StratifiedKFold(n_splits=15, shuffle=True, random_state=36)
x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=42)

pipeline_nb = make_pipeline(MultinomialNB())

grid_nb = GridSearchCV(
    pipeline_nb,
    param_grid={
        'multinomialnb__alpha': [0.1, 0.8,0.01],
        'multinomialnb__fit_prior': [True, False]
    },
    cv=kfolds,
    scoring="accuracy",
    verbose=2,
    n_jobs=-1
)

grid_nb.fit(x_train, y_train)

train_predictions = grid_nb.predict(x_train)
test_predictions = grid_nb.predict(x_test)

print("Classification Report - Training Data:")
print(classification_report(y_train, train_predictions))

print("Classification Report - Test Data:")
print(classification_report(y_test, test_predictions))

train_accuracy = grid_nb.score(x_train, y_train)
test_accuracy = grid_nb.score(x_test, y_test)
print(grid_nb.best_params_)
print("Accuracy - Training Data:", train_accuracy)
print("Accuracy - Test Data:", test_accuracy)

# Plot confusion matrix
categories = ['Negative',  'Neutral','Positive']
cf_matrix = confusion_matrix(y_test, test_predictions)
labels = np.asarray(cf_matrix.flatten()).reshape(3, 3)
sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt='', xticklabels=categories, yticklabels=categories)
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.title("Confusion Matrix")
plt.show()

from sklearn.model_selection import learning_curve
model = MultinomialNB()
x_vectorized = vectorizer.transform(x)
train_sizes, train_scores, val_scores = learning_curve(model, x_resampled, y_resampled, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training accuracy')
plt.plot(train_sizes, val_mean, label='Testing accuracy')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.title('Learning Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# using logistic regression
X_train, X_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=42)
model1 = LogisticRegression(multi_class='ovr')
model1.fit(X_train, y_train)
y_pred1 = model1.predict(X_test)
accuracy1 = accuracy_score(y_test, y_pred1)
print("Accuracy of OvR model:", accuracy1)
print("Classification Report of OvR:","\n",classification_report(y_test,y_pred1))
print("\n")
cm = confusion_matrix(y_test, y_pred1)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d',  cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), sharey=True)

common_params = {
    "X": X_train,
    "y": y_train,
    "train_sizes": np.linspace(0.1, 1.0, 5),
    "cv": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),
    "score_type": "both",
    "n_jobs": 4,
    "line_kw": {"marker": "o"},
    "std_display_style": "fill_between",
    "score_name": "Accuracy",
}
LearningCurveDisplay.from_estimator(log_reg, **common_params, ax=ax)
handles, label = ax.get_legend_handles_labels()
ax.legend(handles[:2], ["Training Score", "Test Score"])
ax.set_title(f"Learning Curve for {model1._class.name_}")

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2', None],
    'solver': ['lbfgs', 'liblinear', 'sag', 'saga']
}
grid_search = GridSearchCV(model1, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)
print("\n")
print("Best parameters:", grid_search.best_params_)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("Classification Report:","\n",classification_report(y_test,y_pred))
print("\n")
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d',  cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# using random forest
from sklearn.ensemble import RandomForestClassifier
X_train, X_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=42)
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)
y_pred = rf_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d',  cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

train_accuracy = []
test_accuracy = []
epochs = 100

model1 = RandomForestClassifier(n_estimators=1, random_state=0, warm_start=True)

for epoch in range(epochs):
  model1.n_estimators+=1
  model1.fit(X_train, y_train)
  y_pred1 = model1.predict(X_train)
  train_accuracy.append(accuracy_score(y_train, y_pred1))

  y_pred2 = model1.predict(X_test)
  test_accuracy.append(accuracy_score(y_test, y_pred2))
plt.plot(np.arange(1, epochs + 1), train_accuracy, 'b-', label='Train acc')
plt.plot(np.arange(1, epochs + 1), test_accuracy, 'r-', label='Test acc')
plt.title('Train and Test Accuracy Curves')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

from sklearn.model_selection import GridSearchCV
n_estimators = [int(x) for x in np.linspace(start =10, stop=80, num=10)]
param_grid = {
    'n_estimators': n_estimators,
    'criterion': ["gini","entropy","log_loss"],
    'max_depth': [None,2,4],
    'min_samples_split': [2,5],
    'min_samples_leaf': [1, 2],
    'warm_start': [True,False],
    'max_features': ['auto', 'sqrt']
}
grid_search = GridSearchCV(rf_classifier, param_grid=param_grid, cv=3)
grid_search.fit(X_train, y_train)
print("\n")
print("Best parameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:","\n",classification_report(y_test,y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d',  cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()